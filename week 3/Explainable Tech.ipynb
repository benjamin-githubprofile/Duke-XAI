{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explainable Technique I**\n",
    "\n",
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Gs7j-IhnBke78j-Afd9BVNZTjYJB6fXT?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dog.jpg was being used during the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "\n",
    "The ResNet34 model correctly identifies the image as a Labrador retriever with a high probability of 93.23%, followed by golden retriever (6.55%), and other unrelated categories like tennis ball, beagle, and kuvasz with negligible probabilities. However, when using LIME for interpretability, the top label predicted by the explainer is incorrectly identified as \"coral reef\" (class index 973), indicating a preprocessing mismatch in the perturbed samples used by LIME. The most influential superpixels in LIME's explanation have relatively low weights (e.g., Superpixel 25: 0.0644), further suggesting inconsistencies in how the image is being processed between normal inference and the LIME explanation pipeline. Additionally, a matplotlib warning about clipping input data to valid RGB ranges hints that the image may have undergone unintended transformations, reinforcing the likelihood of a preprocessing error affecting LIMEâ€™s results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
