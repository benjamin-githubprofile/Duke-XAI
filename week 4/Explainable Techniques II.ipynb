{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection & Model Interpretability\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1QFgAuYFmcEfG-Hy3vAQGFoHepHkPJgm0?usp=sharing)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook we perform the following steps:\n",
    "1. **Exploratory Data Analysis (EDA):** Examine class distribution, correlations, and feature distributions.\n",
    "2. **Preprocessing & Model Training:** Standardize selected features, split data, and train a Random Forest classifier.\n",
    "3. **Model Evaluation & Feature Importances:** Evaluate the model and visualize the top features.\n",
    "4. **Model Interpretation:** Generate Partial Dependence Plots (PDP) with ICE curves and Accumulated Local Effects (ALE) plots.\n",
    "\n",
    "---\n",
    "\n",
    "## Why We Chose These Methods\n",
    "\n",
    "- **Random Forest Classifier:**  \n",
    "  Chosen for its robustness, ability to handle high-dimensional data, and its built-in mechanism for dealing with imbalanced datasets (using `class_weight=\"balanced\"`).\n",
    "\n",
    "- **PDP with ICE Curves:**  \n",
    "  Partial Dependence Plots (PDPs) provide a clear view of the average effect of a feature on model predictions. ICE curves supplement this by showing the variability in predictions across individual instances.  \n",
    "  *However, note that PDPs may be misleading when features are correlated.*\n",
    "\n",
    "- **Accumulated Local Effects (ALE) Plots:**  \n",
    "  ALE plots overcome some of PDPs’ limitations by computing the local effect of a feature on predictions, making them more reliable in the presence of correlated features.\n",
    "\n",
    "---\n",
    "\n",
    "## Strengths of Our Approach\n",
    "\n",
    "- **Random Forests:**  \n",
    "  - Robust to overfitting.\n",
    "  - Naturally handle nonlinear relationships.\n",
    "  - Easily interpretable through feature importance.\n",
    "\n",
    "- **PDP/ICE Visualizations:**  \n",
    "  - Show both global (PDP) and local (ICE) feature effects.\n",
    "  - Easy to understand when features are nearly independent.\n",
    "\n",
    "- **ALE Plots:**  \n",
    "  - Unbiased in the presence of feature correlations.\n",
    "  - Provide a more realistic view of feature influence compared to PDPs in many real-world datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Potential Improvements\n",
    "\n",
    "- **Hyperparameter Tuning:**  \n",
    "  Optimize the Random Forest’s hyperparameters (e.g., tree depth, number of features per split) for potentially better performance.\n",
    "\n",
    "- **Additional Interpretability Tools:**  \n",
    "  Incorporate alternative model explanation techniques such as SHAP or LIME for further insight.\n",
    "\n",
    "- **Handling Imbalance:**  \n",
    "  Experiment with resampling techniques (e.g., SMOTE) to further address class imbalance.\n",
    "\n",
    "- **Extended EDA:**  \n",
    "  Deepen the exploratory analysis (e.g., using additional visualization or statistical tests) to better understand the data.\n",
    "\n",
    "---\n",
    "\n",
    "## Citations\n",
    "\n",
    "1. Molnar, C. (2020). *Interpretable Machine Learning*. Retrieved from [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)\n",
    "2. Apley, D. W., & Zhu, J. (2020). Visualizing the effects of predictor variables in black box supervised learning models. *Journal of the Royal Statistical Society: Series B*, 82(4), 1059–1086.\n",
    "3. Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. *Journal of Machine Learning Research*, 12, 2825–2830.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install numpy pandas matplotlib seaborn scikit-learn PyALE scikit-learn-intelex -q\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from PyALE import ale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from google.colab import drive, files\n",
    "\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict(self, X):\n",
    "        return self.model.predict_proba(X)[:, 1]\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "file_path = \"/content/drive/MyDrive/creditcard.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"Class distribution:\\n\", df[\"Class\"].value_counts(normalize=True))\n",
    "\n",
    "corr_matrix = df.drop(\"Time\", axis=1).corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(corr_matrix, vmin=-0.1, vmax=0.1, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "sns.histplot(df[\"Time\"], ax=ax[0], bins=50, kde=True)\n",
    "sns.histplot(df[df[\"Amount\"] < 2000][\"Amount\"], ax=ax[1], bins=50, kde=True)\n",
    "plt.suptitle(\"Feature Distributions\")\n",
    "plt.show()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[[\"Time\", \"Amount\"]] = scaler.fit_transform(df[[\"Time\", \"Amount\"]])\n",
    "\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "print(classification_report(y_test, preds))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, model.predict_proba(X_test)[:,1]):.4f}\")\n",
    "\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "importances = importances.sort_values(ascending=False)[:5]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "importances.plot(kind=\"bar\")\n",
    "plt.title(\"Top 5 Feature Importances\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    model,\n",
    "    X_train,\n",
    "    features=[\"V17\"],\n",
    "    kind=\"both\", \n",
    "    ice_lines_kw={\"color\":\"tab:blue\", \"alpha\":0.2, \"linewidth\":0.3},\n",
    "    pd_line_kw={\"color\":\"red\", \"linewidth\":3},\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title(\"PDP/ICE Plot for V17\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    model,\n",
    "    X_train,\n",
    "    features=[\"Amount\"],\n",
    "    kind=\"both\",\n",
    "    ice_lines_kw={\"color\":\"tab:blue\", \"alpha\":0.2, \"linewidth\":0.3},\n",
    "    pd_line_kw={\"color\":\"red\", \"linewidth\":3},\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title(\"PDP/ICE Plot for Amount\")\n",
    "plt.show()\n",
    "\n",
    "X_train_sample = X_train.sample(n=5000, random_state=42)\n",
    "\n",
    "wrapper = ModelWrapper(model)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ale(\n",
    "    X=X_train_sample,\n",
    "    model=wrapper,\n",
    "    feature=[\"V17\"],\n",
    "    grid_size=50\n",
    ")\n",
    "plt.title(\"ALE Plot for V17 (Subsampled Data)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ale(\n",
    "    X=X_train_sample,\n",
    "    model=wrapper,\n",
    "    feature=[\"Amount\"],\n",
    "    grid_size=50\n",
    ")\n",
    "plt.title(\"ALE Plot for Amount (Subsampled Data)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
